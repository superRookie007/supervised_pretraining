{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(n_instances, n_features, n_informative_features, label_noise, seed=0, n_classes=2, n_clusters=1):\n",
    "    \"Generate synthetic data.\"\n",
    "    X, y = datasets.make_classification(\n",
    "        n_samples=n_instances,                  # The number of samples\n",
    "        n_features=n_features,                  # The total number of features.\n",
    "        n_informative=n_informative_features,   # The number of informative features\n",
    "        n_redundant=0,                          # No redundant features\n",
    "        n_repeated=0,                           # No duplicated features\n",
    "        n_classes=n_classes,                    # The number of classes\n",
    "        n_clusters_per_class=n_clusters,        # The number of clusters per class.\n",
    "        weights=None,                           # balanced classes\n",
    "        flip_y=label_noise,                     # The fraction of samples whose class are randomly exchanged\n",
    "        class_sep=1.0,                          # Larger values spread out the clusters/classes and make the classification task easier.\n",
    "        scale=1.0,                              # No scale\n",
    "        shuffle=True,                           # Shuffle the samples and the features.\n",
    "        random_state=seed)\n",
    "    \n",
    "    # attach labels to data\n",
    "    data = np.column_stack((X,y))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_test(data, test_size, seed=0):\n",
    "    \"\"\"\n",
    "    Create a test set from the generated synthetic data.\n",
    "    \n",
    "    Args:\n",
    "        data: numpy array. \n",
    "        test_size: float or int\n",
    "        seed: int.\n",
    "        \n",
    "    Return:\n",
    "        test set and the remaining data (both are numpy arrays)\n",
    "    \"\"\"\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    for remain_index, test_index in sss.split(data, data[:,-1]):\n",
    "        test_set = data[test_index]\n",
    "        remain = data[remain_index]   \n",
    "        \n",
    "    return test_set, remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stratified_split(data, train_size, seed=0):\n",
    "    \"\"\"\n",
    "    stratified sampling to create training set of desired size.\n",
    "    \n",
    "    Args:\n",
    "        data: numpy array.\n",
    "        train_size: float or int\n",
    "        seed: int.\n",
    "    \n",
    "    Return:\n",
    "        A numpy array containing training data.\n",
    "    \"\"\"\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, test_size=None, random_state=seed)\n",
    "    for train_index, test_index in sss.split(data, data[:,-1]):\n",
    "        training_set = data[train_index]\n",
    "        test_set = data[test_index]\n",
    "        \n",
    "    return training_set, test_set   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets with different number of instances, but no noisy features or noisy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_INSTANCES = 30000\n",
    "N_FEATURES = 25\n",
    "N_INFORMATIVE = 25 \n",
    "# number of noisy features = N_FEATURES - N_INFORMATIVE\n",
    "LABEL_NOISE = 0.0\n",
    "SEED=0\n",
    "\n",
    "DIR_PATH = \"/home/alex/gitrepos/project_activation_function/generalisation/synthetic/size\"\n",
    "\n",
    "N_CLASSES = 2\n",
    "N_CLUSTERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "data = generate(\n",
    "    n_instances=N_INSTANCES, \n",
    "    n_features=N_FEATURES, \n",
    "    n_informative_features=N_INFORMATIVE, \n",
    "    label_noise=LABEL_NOISE, \n",
    "    seed=SEED, \n",
    "    n_classes=N_CLASSES, \n",
    "    n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and write test data to a file\n",
    "TEST_SIZE = 5000\n",
    "\n",
    "test_set, remain = create_test(data, test_size=TEST_SIZE)\n",
    "\n",
    "FILENAME = \"test_set.csv\"\n",
    "WRITE_PATH = DIR_PATH + \"/\" + FILENAME\n",
    "# save test set to csv file\n",
    "np.savetxt(WRITE_PATH, test_set, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training sets with different sizes and write them to files\n",
    "TRAIN_SIZE = [100, 500, 1000, 5000, 10000]\n",
    "\n",
    "for train_size in TRAIN_SIZE:\n",
    "    train_set, _ = stratified_split(remain, train_size, seed=0)\n",
    "    \n",
    "    FILENAME = \"{}_instances.csv\".format(train_size)\n",
    "    WRITE_PATH = DIR_PATH + \"/\" + FILENAME\n",
    "    # save test set to csv file\n",
    "    np.savetxt(WRITE_PATH, train_set, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing number of noisy features but fixed number of training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_INSTANCES = 10000\n",
    "N_FEATURES = 25\n",
    "# create training sets and test sets with different number of noisy features and write them to files\n",
    "# N_INFORMATIVE = [25, 20, 15, 10, 5]\n",
    "N_INFORMATIVE = [0]\n",
    "\n",
    "# number of noisy features = N_FEATURES - N_INFORMATIVE\n",
    "TRAIN_SIZE = 5000\n",
    "LABEL_NOISE = 0.0\n",
    "SEED=0\n",
    "\n",
    "DIR_PATH = \"/home/alex/gitrepos/project_activation_function/generalisation/synthetic/noise_feature\"\n",
    "\n",
    "N_CLASSES = 2\n",
    "N_CLUSTERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and write test data to a file\n",
    "TEST_SIZE = 5000\n",
    "\n",
    "for n_informative in N_INFORMATIVE:\n",
    "    # Generate synthetic data\n",
    "    data = generate(\n",
    "        n_instances=N_INSTANCES, \n",
    "        n_features=N_FEATURES, \n",
    "        n_informative_features=n_informative, \n",
    "        label_noise=LABEL_NOISE, \n",
    "        seed=SEED, \n",
    "        n_classes=N_CLASSES, \n",
    "        n_clusters=N_CLUSTERS)\n",
    "    \n",
    "    train_set, test_set = stratified_split(data, TRAIN_SIZE, seed=0)\n",
    "    \n",
    "    FILENAME = \"{}_noisy_features.csv\".format(N_FEATURES-n_informative)\n",
    "    WRITE_PATH = DIR_PATH + \"/\" + FILENAME    \n",
    "    # save data to csv file\n",
    "    np.savetxt(WRITE_PATH, train_set, delimiter=\",\")\n",
    "    \n",
    "    FILENAME = \"{}_noisy_features_test.csv\".format(N_FEATURES-n_informative)\n",
    "    WRITE_PATH = DIR_PATH + \"/\" + FILENAME    \n",
    "    # save data to csv file\n",
    "    np.savetxt(WRITE_PATH, test_set, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing label noises, no noisy features, fixed number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_INSTANCES = 10000\n",
    "N_FEATURES = 25\n",
    "# create training sets and test sets with different number of noisy features and write them to files\n",
    "N_INFORMATIVE = 25\n",
    "# number of noisy features = N_FEATURES - N_INFORMATIVE\n",
    "TRAIN_SIZE = 5000\n",
    "LABEL_NOISE = [0.0, 0.1, 0.01, 0.001]\n",
    "SEED=0\n",
    "\n",
    "DIR_PATH = \"/home/alex/gitrepos/project_activation_function/generalisation/synthetic/noise_label\"\n",
    "\n",
    "N_CLASSES = 2\n",
    "N_CLUSTERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create and write test data to a file\n",
    "TEST_SIZE = 5000\n",
    "\n",
    "for label_noise in LABEL_NOISE:\n",
    "    # Generate synthetic data\n",
    "    data = generate(\n",
    "        n_instances=N_INSTANCES, \n",
    "        n_features=N_FEATURES, \n",
    "        n_informative_features=N_INFORMATIVE, \n",
    "        label_noise=label_noise, \n",
    "        seed=SEED, \n",
    "        n_classes=N_CLASSES, \n",
    "        n_clusters=N_CLUSTERS)\n",
    "    \n",
    "    train_set, test_set = stratified_split(data, TRAIN_SIZE, seed=0)\n",
    "    \n",
    "    FILENAME = \"{}_noisy_labels.csv\".format(str(label_noise).replace('.',''))\n",
    "    WRITE_PATH = DIR_PATH + \"/\" + FILENAME    \n",
    "    # save data to csv file\n",
    "    np.savetxt(WRITE_PATH, train_set, delimiter=\",\")\n",
    "    \n",
    "    FILENAME = \"{}_noisy_labels_test.csv\".format(str(label_noise).replace('.',''))\n",
    "    WRITE_PATH = DIR_PATH + \"/\" + FILENAME    \n",
    "    # save data to csv file\n",
    "    np.savetxt(WRITE_PATH, test_set, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(path):\n",
    "    \"\"\"\n",
    "    Read data in csv format.\n",
    "    Note: this function should only be used for dataset that can fit into memory.\n",
    "\n",
    "    Args:\n",
    "        path: a string that specifies the path to the csv file\n",
    "\n",
    "    Return:\n",
    "        data as numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    # load raw data into memory\n",
    "    data = pd.read_csv(path, header=None)\n",
    "    \n",
    "    # find the target index from the target location index\n",
    "    target_index = data.columns[-1]\n",
    "\n",
    "    labels = pd.get_dummies(data[target_index])\n",
    "    data.drop(target_index, axis=1, inplace=True)\n",
    "\n",
    "    return data.as_matrix(), labels.as_matrix()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
